---
layout: post
title: "O preço do futuro"
subtitle: A vida num tubo de ensaio 
collection: blog_posts
date: 2023-03-15
description: 
permalink: /blog_posts/ratos-laboratorio
author_profile: true
sidebar: true
tags: introducao
---

No dia 14 de Março de 2023, o novo modelo da OpenAI, GPT-4, [acaba de ser disponibilizado ao público](https://openai.com/product/gpt-4). Tal como o seu irmão mais velho, GPT-3.5, é um modelo de previsão de texto treinado com quantidades enormes de texto. Ao contrário do seu irmão mais velho, o GPT-4 é também treinado com imagens. Tal como os seus precedentes, faz parte de uma cadeia de produção e experimentação com novos métodos de inteligência artificial e a sua validação e melhoramento em tempo real. Mas o que é que tudo isto implica e até que ponto estamos preparados para o consentir?

# Bê-Á-Bá - aprender a falar e aprender a conversar

O "treino" (a maneira como ensinamos algoritmos a realizar determinadas tarefas) do [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5) é relativamente simples - dado um conjunto de palavras, este algoritmo tem de prever a(s) palavra(s) que se seguem. Por exemplo, para o excerto "Chamava-se Catarina", seria de esperar que o GPT-3.5 produzisse uma sequência de texto como "O Alentejo a viu nascer" como na popular música de Zeca Afonso sobre o assassinato de Catarina Eufêmia pela PIDE, _Cantar Alentejano_. 

Contudo, o conjunto de textos (também conhecido como _corpus_) usado para treinar o GPT-3.5 foram cegamente recolhidos da internet. O que isto implica é que uma parte considerável do texto produzido por estes algoritmos é ofensiva, sendo que em bastantes casos pode até constituir discurso de ódio. Como uma curiosidade capaz de gerar texto até que podia ser algo interessante, mas não podia ser levado até ao público com leviandade. Mais importantemente (para uma empresa): não podia ser comercializável. 

A solução para isto jaz numa técnica específica - aprendizagem por reforço com feedback humano (ou RLHF, do inglês [_reinforcement learning with human feedback_](https://arxiv.org/abs/2203.02155)). Trocado por miúdos - uma série de humanos vê um par de frases produzidas pelo GPT-3.5 e decide qual das duas é mais aceitável de acordo com critérios como "toxicidade" (definida largamente como texto ofensivo), veracidade ou coerência. Isto dá origem a modelos com os quais pessoas podem interagir em grande escala e - _voilà_ - assim nasce o ChatGPT. 

# Dados - conceção divina ou origem oculta?

Contudo, ficou aqui algo por explicar - quem gera estes dados? Quem é que tem de olhar para conteúdo ofensivo e tóxico para ensinar ao modelo a falar com pessoas de forma aceitável? [Documentos recentes](https://time.com/6247678/openai-chatgpt-kenya-workers/) reveleram que foram trabalhadores quenianos, pagos com um valor máximo de 2$ por hora, os responsáveis por ver conteúdo ofensivo e potencialmente traumático. A empresa responsável por esta tarefa de anotação é a Sama, baseada em São Francisco e conhecida por contratar trabalhadores no Quénia, Uganda e India para anotar dados para Silicon Valley. O contrato estipulado foi de um ano, mas a Sama teve de interromper o trabalho de anotação de dados devido à natureza traumatizante do trabalho oito meses antes do planeado. 

Pode parecer estranho, mas este tipo de abordagens - em que grandes companhias usam serviços sub-contratados para anotar quantidades massivas de dados - não é incomum. O [Mechanical Turk](https://marketing4ecommerce.net/mechanical-turk/) [^turk] é um serviço disponibilizado da Amazon que descentraliza esta actividade: coloca, nas mãos de indivíduos sem uma verificação particular de credenciais, a responsabilidade de anotar estes dados. Estas actividades, sem grande surpresa, são lideradas por pessoas marginalizadas - um [estudo realizado sobre pessoas que trabalham para o Mechanical Turk nos EUA](https://www.cloudresearch.com/resources/blog/who-uses-amazon-mturk-2020-demographics/) revelou que são tipicamente mulheres que ganham abaixo da média. No geral, este tipo de "micro-trabalhos" pode ser problemático de várias maneiras - [para trabalho no Mechanical Turk, empresas como a Sama ou a Cloud Factory empregam refugiados](https://digilabour.com.br/en/microwork-in-the-global-south-the-motor-of-platform-capitalism-interview-with-phil-jones/), pessoas que tipicamente não têm acesso aos direitos do cidadão comum e se vêem obrigados a trabalhar em posições que não são necessariamente vantajosas. Num [questionário elaborado pela Gizmodo](https://gizmodo.com/horror-stories-from-inside-amazons-mechanical-turk-1840878041), 1 em cada 10 trabalhadores no Mechanical Turk afirma que a sua pior experiência foi ver uma imagem de conteúdo gráfico - decapitações, abuso animal, suicídios ou pornografia infantil. Tanto quanto sabemos, empresas como a Facebook continuam a pagar para que [seres humanos vejam conteúdo traumatizante](https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona) (e até potencialmente ilegal) como pornografia ou _snuff_ (vídeos de agressões, violaçôes ou assassinatos consumidos para entretenimento).

O trabalho muitas vezes traumático e sempre ignorado de pessoas marginalizadas no desenvolvimento de novas tecnologias tem um longo historial. Este incidente com os trabalhadores quenianos é só mais um tijolo no longo muro que divide a sociedade entre aqueles que podem escolher entre serem ou não expostos a traumas, entre aqueles que podem escolher ter uma vida segura e aqueles que ficam relegados a aceitar as parcas oportunidades que as suas circunstâncias podem oferecer.

Neste ponto, é impossível não perguntar algo bastante concreto - porque é que estes conjuntos de dados usados para treinar modelos que vão ser usados por números enormes de pessoas contêm conteúdo gráfico ou até ilegal? A resposta é simples - como referi anteriormente, estes dados são recolhidos cegamente da internet. Se, por acaso, um site hospedar conteúdo sexual com crianças (como é o caso de [várias redes sociais e sites pornográficos](https://www.consumeraffairs.com/news/social-media-and-porn-sites-are-working-together-to-remove-sexually-explicit-photos-of-children-online-030923.html)), esse conteúdo vai parar a essas bases de dados. A equação é simples - se está na internet pode ser usado para treinar estes modelos. É verdade que esta tendência está em vias de mudar [^stable-diffusion-litigation], mas o mal está feito.

O trabalho de Abeba Birhane, cientista cognitiva etíope, incidiu sobre este tópico problemático. Ao analisar milhões de imagens nalgumas bases de dado recolhidoas cegamente da internet, revelou que bases de dados multi-modais[^multi-modal], semelhantes àqueles usados para treinar o GPT-4, [contêm bastante conteúdo ofensivo, ilegal ou pornográfico](https://arxiv.org/pdf/2110.01963.pdf). Num artigo de divulgação para a [Nature](https://www.nature.com/articles/d41586-022-03050-7), a mesma cientista fala-nos sobre nova investigação que mostra que modelos treinados para identificar caras têm um desempenho consideravelmente pior em pessoas negras, uma investigação avançada primeiramente em 2018 por [Joy Buolamwini e Timnit Gebru](https://proceedings.mlr.press/v81/buolamwini18a.html)[^gebru]. Amplas provas que demonstram os vieses discriminatórios também já foram apresentadas para modelos de conversação - há evidência de que modelos da família GPT e o próprio ChatGPT podem ser [sexistas](https://aclanthology.org/2021.nuse-1.5/), [racistas](https://theintercept.com/2022/12/08/openai-chatgpt-ai-bias-ethics/), [homofóbicos ou capacitistas](https://www.cbsnews.com/news/chatgpt-large-language-model-bias-60-minutes-2023-03-05/). 

Estes algoritmos, criados para automatizar e facilitar tarefas humanas como identificação facial ou recolha e busca de conhecimento de forma "objectiva", acabam por recriar discriminações que humanos fazem entre si - a recolha, anotação e refinamento destes dados usados para treinar estes algoritmos aproveita-se da situação precária em que muitas pessoas marginalizadas vivem. Mesmo que os investigadores tenham as melhores das intenções, a os dados usados têm um historial forte de discriminação e isso cria modelos inerentemente discriminatórios. A questão que se impõe, no final de contas, é simples - será que o preço do progresso humano se traduzirá, ultimamente, no reforço daquilo que consideramos desumano? Por outras palavras: vale a pena criar novos modelos e formas daquilo que entendemos como "inteligência artificial" se isso implica que destruamos mais vidas?

# A crueldade humana em nome da humanidade

Quando queremos falar sobre a maneira como pessoas são forçadas a serem sujeitos de teste por uma ideia (muitas vezes abstracta) de progresso tecnológico, o passado torna-se numa fonte informativa. Há, como é certo e sabido, um historial consistente de escravatura ou outras formas de trabalho forçado que [precedem a Grécia antiga](http://www.ditext.com/moral/slavery.html) por milénios. Contudo, apesar desta ser uma questão importante, eu vou focar-me aqui noutras formas de trabalho forçado ou crueldade, naquelas que foram justificadas com um propósito supostamente nobre - o desenvolvimento tecnológico e/ou científico.

Talvez os casos de experimentação em humanos melhor conhecidos serão aqueles que se passaram na [Alemanha na altura do terceiro Reich](https://encyclopedia.ushmm.org/content/en/article/nazi-medical-experiments), quando estava sobre o controlo do partido nazi. Nestas experiências, invariavelmente realizadas em pessoas marginalizadas, particularmente pessoas de etnia judaica ou Roma. Desde esterilização forçada em massa até incubação de doenças em humanos para determinar como diferentes "raças" lidavam com infeções, os exemplos não são poucos e constituem uma extensão natural da atitude desumanizante com que os nazis encaravam o mundo.

Contudo, falar das experiências forçadas na Alemanha nazi como algo único seria evidentemente errado - afinal de contas, a inspiração para os programas de eugenia alemães [veio do outro lado do Atlântico](https://www.theatlantic.com/magazine/archive/2017/11/what-america-taught-the-nazis/540630/). Durante o século XX - desde 1921 até à década de 70 para ser mais preciso - os EUA tiveram vários programas de eugenia em que não só esterilizavam pessoas em massa contra o seu consentimento (muitos destes eram considerados de "raças inferiores" ou eram vistos como inferiores intelectuais ou mentais) mas também proibiam os casamentos entre pessoas de diferentes raças. Outras experiências forçadas dos EUA - como as [experiências de Tuskegee](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1780164/) (1932-1972), em que 600 afro-americanos foram infectados com sífilis sem o saberem para testar sem sucesso potenciais curas, ou o [MKUltra](https://www.cs.mcgill.ca/~rwest/wikispeedia/wpcd/wp/p/Project_MKULTRA.htm) (1953-1973), o programa da CIA que tentava controlar mentes a partir do uso de drogas, radiação ou sinais electromagnéticos - contribuem para aquilo que é um historial considerável de desumanização em nome de um "bem maior".

Ao longo desse período, outras nações "ocidentais" também se dedicaram a esta forma de "desumanização científica". No Canadá nos anos 30, [600 crianças indígenas foram propositadamente infectadas com tuberculose](https://utpjournals.press/doi/10.3138/cbmh.15.2.277) para que uma vacina fosse testada nelas, acabando por morrer um quinto delas devido às más condições a que eram sujeitas. Na Austrália, também nos anos 30, pessoas aborígenes foram sujeitas a testes e, efectivamente torturados, para determinar como [respondiam à dor](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1125797/). 

No Reino Unido durante a segunda guerra mundial, [diferentes formulações de gás mostarda foram testadas em soldados coloniais britânicos e indianos](https://www.theguardian.com/uk/2007/sep/01/india.military), para que pudessem compreender melhor qual delas era mais eficaz contra diferentes cores de pele. Entre 1940 e 1979, [vários agentes patogénicos ou químicos foram espalhados pelo Reino Unido pelo próprio ministério da defesa](https://www.theguardian.com/politics/2002/apr/21/uk.medicalscience), para que o país pudesse testar a sua capacidade de resposta contra um potencial ataque químico ou biológico por parte da União Soviética. Efectivamente, o Reino Unido usou potencialmente milhões de pessoas - a esmagadora maioria dos quais era britânicos - para testes.  Para lá daquilo que conhecemos hoje como o "ocidente", houve ainda [casos de experimentação humana na União Soviética](https://en.wikipedia.org/wiki/Poison_laboratory_of_the_Soviet_secret_services), onde venenos eram desenvolvidos e forçosamente testados em humanos nos Gulags - a ideia era encontrar um veneno que não tivesse cheiro, sabor e que não fosse detectado numa autópsia.

Há uma forte história de desumanização e experimentação humana forçada em nome de um qualquer bem maior. A esmagadora maiora das nações parece ter-se visto livres destas formas de experimentação. Mas a realidade não é assim tão simples - empresas e estados continuam a desenvolver, testar e usar ferramentas tecnológicas em centenas de milhões de pessoas. Para que isso acontecesse, o laboratório deixou de ser uma sala estéril - passou a ser o mundo digital.

# Quem se serve dos termos de serviço?

Começo esta secção por ser claro: não quero aqui sugerir que ser alvo de testes que não colocam em risco o nosso corpo é equivalente a testes que forçosamente mataram ou debilitaram severamente pessoas; uma admissão dessa ordem seria desonesta e ofensiva. Tal como não quero sugerir que este tipo de experiências tenham cessado por completo. Em vez disso, a minha sugestão é mais leviana - a tendência de fazer experiências em pessoas sem que as próprias dêem o seu consentimento explícito transformou-se e tornou-nos praticamente a todos, ao longo dos anos, em sujeitos de teste.

Antes, esta ausência de consentimento era desconsiderada - não havia sequer uma tentativa de o recolher. Agora, o consentimento tornou-se num assunto complicado. Enquanto que procedimentos e ensaios clínicos requerem que os indivíduos que se sujeitam a eles sejam devidamente informados, isto não acontece quando navegamos na web ou usamos programas de computador - basta clicarmos num botão. Os termos de serviço com os quais concordamos através deste clique estão muitas vezes carregados de jargão técnico e legal, tornando-os inacessíveis a muita gente. Para além disto, podem ser [ridiculamente longos](https://www.socialmediatoday.com/news/how-long-does-it-take-to-read-the-terms-of-service-for-each-app-infograph/577235/). Há uma diferença entre o consentimento que damos quando "concordamos" com termos de serviço sem os ler - consentimento expresso - do consentimento que tomamos quando estamos devidamente informados - consentimento informado. Na esmagadora maioria dos casos (onde me incluo), muita gente não saberá que dados cada companhia tem sobre eles. Num relatório elaborado pela União Europeia, [apenas 15% dos utilizadores da internet sentiam que tinham poder sobre a informação que depositaram online](https://www.delta-net.com/knowledge-base/compliance/gdpr/what-is-gdpr-in-simple-terms/). Há um vale colossal entre o botão que diz "Concordo" e o nosso consentimento. Isto leva a problemas.

Em 2018, soam os primeiros alarmes sobre aquele que foi um dos maiores escândalos de manipulação em massa - o uso de dados do Facebook pela Cambridge Analytica para manipular [as eleições presidenciais nos EUA em 2016](https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html), [o referendo sobre o Brexit](https://www.dw.com/en/what-role-did-cambridge-analytica-play-in-the-brexit-vote/a-43151460) ou [as eleições presidenciais de 2015 na Nigéria](https://www.theguardian.com/world/2023/feb/16/team-jorge-and-cambridge-analytica-meddled-in-nigeria-election-emails-reveal). Com dados sobre milhões de indivíduos nestes países, a Cambridge Analytica foi capaz de criar publicações, campanhas de desinformação e anúncios em redes sociais capazes de influenciar votos. A recolha destes dados foi extremamente simples e ninguém se apercebeu dela - um utilizador do Facebook apenas tinha de consentir a que uma aplicação dentro do site da rede social pudesse recolher os seus dados e os dados dos seus amigos a que o utilizador tinha acesso. Isto permitiu criar uma base de dados enorme sobre milhões de indíviduos.

Apesar desta recolha de informação parecer aparentemente inócua, há algo importante a referir - o Facebook é capaz de prever os interesses que nós temos. Em 2013, um trabalho controverso mostrou que havia a possibilidade de [prever traços de personalidade a partir de likes do Facebook](https://www.bbc.com/news/technology-21699305) (a Facebook continua a categorizar pessoas de acordo com os seus tópicos de interesse para efeitos de publicidade). A partir de pouca informação, podemos estar a revelar ao Facebook e à Meta aspectos da nossa vida que nos são relativamente privados como as nossas tendências políticas. Foi este tipo de informação - obtida com alguma forma de consentimento - que a Cambridge Analytica usou para manipular o voto em três países. Em 2020, dois anos depois do escândalo da Cambridge Analytica se ter tornado público, Sophie Zhang, antiga cientista de dados na Facebook mostrou que a empresa continua a permitir que as suas plataformas sejam usadas para [manipulação de eleições em vários países](https://www.technologyreview.com/2021/07/29/1030260/facebook-whistleblower-sophie-zhang-global-political-manipulation/), fazendo pouco ou nada para o parar. No Twitter, houve também esforços coordenados para [influenciar eleições nos EUA em 2016](https://www.theguardian.com/technology/2018/jan/19/twitter-admits-far-more-russian-bots-posted-on-election-than-it-had-disclosed), e a plataforma parece ter tido problemas recentes em [controlar a maneira como desinformação é espalhada](https://www.reuters.com/technology/elon-musks-twitter-girds-surge-us-midterm-election-misinformation-2022-11-08/). 

Quando decidimos participar em plataformas como a Facebook ou o Twitter, estamos a consentir com estes aspectos da plataforma? Estamos a concordar com as campanhas de desinformação ou com a recolha dos nossos dados? Estamos a concordar com experiências psicológicas, como aquelas feitas pela Facebook em 2012 quando [removeu certas palavras do feed de algumas pessoas para ver como isso alterava os seus gostos](https://www.theguardian.com/technology/2014/jul/02/facebook-apologises-psychological-experiments-on-users)? Estamos a concordar com o Facebook quando permite que o genocídio de [muçulmanos Rohyngia no Myanmar seja incitado](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html) ou com o WhatsApp/Facebook quando levam a [violência organizada e mortífiera contra muçulmanos na Índia](https://maktoobmedia.com/india/facebook-enabled-violence-against-muslims-in-india-internal-documents-show/)? Estamos a concordar com o Twitter quando permite que apoiantes do ditador sírio Bashar al-Assad possam [pôr em causa a utilização de ataques químicos pelo exército sírio](https://reliefweb.int/report/world/peoples-under-threat-2019-role-social-media-exacerbating-violence)?

Em larga escala, quando concordamos com estes termos de serviço não estamos a concordar com a permissibilidade destas ações - pelo menos não explicitamente ou de forma consciente. Mas na ausência de regulação e capacidade de monitorização, estas plataformas existem apenas como amplificadores daquilo que é mais extremo no discurso humano. Pior do que isso, podem ser instrumentalizadas. Os motivos são evidentes - desde o Twitter de Elon Musk que tem de se tornar (de qualquer maneira possível) numa [plataforma economicamente viável](https://arstechnica.com/tech-policy/2022/11/musk-floats-several-gimmicks-to-make-twitter-profitable/) até ao Facebook de Mark Zuckerberg que [ignora o bem estar das pessoas a favor de potenciais lucros](https://www.businessinsider.com/facebook-employees-worrried-about-2018-algorithm-change-misinformation-trump-politics-2021-9), mais utilizadores significa mais dinheiro. Em 2022, a publicidade destas plataformas gerou [90% das receitas do Twitter](https://www.businessofapps.com/data/twitter-statistics/) e [97.5% das receitas do Facebook](https://www.oberlo.com/statistics/facebook-ad-revenue). Mesmo que o Twitter mude o seu modelo de negócio para um baseado em subscrições, o facto fundamental continua o mesmo: mais utilizadores que passam mais tempo na plataforma é bom. E sabemos de estudos recentes que conteúdo que provoque emoções fortes é mais [partilhado e leva a que os utilizadores passem mais tempo nestas plataformas](https://www.frac.tl/work/marketing-research/emotional-content-drive-engagement-social-media/).

Mas que humanidade é que será desenvolvida? Que humanidade nos restará após esse desenvolvimento?

# Por fim, o final - o que pensar sobre o GPT-4?

https://twitter.com/cHHillee/status/1635790330854526981

[^turk]: o nome "Mechanical Turk" vem de um engenho construído a pedido da imperatriz Maria Teresa no século XVIII - uma máquina que jogava xadrez. Contudo e apesar daquilo que os utilizadores desta máquina achavam, dentro desta máquina não havia mecanismos complexos: [havia apenas um homem que operava o movimento das peças](https://www.pewresearch.org/internet/2016/07/11/what-is-mechanical-turk).

[^stable-diffusion-litigation]: felizmente, há já alguns esforços consideráveis para moderar a recolha não-consensual de material da internet ao processar companhias como a [Stability AI, DeviantArt e MidJourney](https://stablediffusionlitigation.com/) por terem criado ou disponibilizado dados para um modelo de geração de imagens sem o consentimento dos artistas ou a [Microsoft](https://githubcopilotlitigation.com/) por criar um modelo de geração de código que usa milhões de linhas de código recolhidas sem o consentimento de quem as escreveu.

[^multi-modal]: a definição de "dados multi-modais" torna-se mais simples se definirmos o que são estes "modos". Um modo é "imagens", outro modo é "texto", outro é "áudio" e por aí adiante - bases de dados multi-modais contêm mais do que um modo (texto e imagem, por exemplo).

[^gebru]: O nome "Timnit Gebru" pode ser familiar para algumas pessoas - trata-se da investigadora negra e ex-responsável pela equipa de inteligência artificial ética que a Google despediu por [publicar um artigo que criticava a Google](https://www.nytimes.com/2020/12/09/technology/timnit-gebru-google-pichai.html).